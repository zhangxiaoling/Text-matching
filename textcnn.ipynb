{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "A5BD7CD63FF3400F9F966B82B36360F3",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 显示cell运行时长\n",
    "%load_ext klab-autotime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "93220B4A99D244D089D01CDE1DCD3E33",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.25 s\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import scipy.stats as stats\n",
    "import pandas as df\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "486A31C3CC3241959336213535316C08",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.33 s\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Input, Dense, Multiply,Flatten,RepeatVector,Permute,Dropout\n",
    "from keras.layers import Conv1D,Bidirectional,LSTM\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "F019A9C981264EDC81BF214E19C6DD5F",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3.72 ms\n"
     ]
    }
   ],
   "source": [
    "def reduce_mem_usage(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "AF56D7146E844B97A5CB58D6C8DFFD0D",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 268 ms\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import scipy.stats as stats\n",
    "import pandas as pd\n",
    "import gc\n",
    "import numpy as np\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "from sklearn.externals import joblib\n",
    "from gensim.models import Doc2Vec\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "FF70D31DB42940A1BB82FDF57DFF5A3E",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 2098.08 Mb (45.0% reduction)\n",
      "time: 14min 2s\n"
     ]
    }
   ],
   "source": [
    "Data_path='/home/kesci/input/bytedance/'\n",
    "train_path=os.path.join(Data_path,\"train_final.csv\")\n",
    "df_file=df.read_csv(train_path,header=None,skiprows=900000000,nrows =100000000)\n",
    "df_file = reduce_mem_usage(df_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "FEF8F17D64D74C1383BF8849D260B192",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 912 µs\n"
     ]
    }
   ],
   "source": [
    "df_file.columns=[\"query_id\",\"query\",\"query_title_id\",\"title\",\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "48E42924F4104FAF8E131E83E4ABE684",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 7.85 ms\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "\n",
    "def train_word2vec(documents, embedding_dim):\n",
    "    \"\"\"\n",
    "    train word2vector over traning documents\n",
    "    Args:\n",
    "        documents (list): list of document\n",
    "        embedding_dim (int): outpu wordvector size\n",
    "    Returns:\n",
    "        word_vectors(dict): dict containing words and their respective vectors\n",
    "    \"\"\"\n",
    "    model = Word2Vec(documents, workers=16,min_count=1, size=embedding_dim)\n",
    "    word_vectors = model.wv\n",
    "    del model\n",
    "    return word_vectors\n",
    "\n",
    "\n",
    "def create_embedding_matrix(tokenizer, word_vectors, embedding_dim):\n",
    "    \"\"\"\n",
    "    Create embedding matrix containing word indexes and respective vectors from word vectors\n",
    "    Args:\n",
    "        tokenizer (keras.preprocessing.text.Tokenizer): keras tokenizer object containing word indexes\n",
    "        word_vectors (dict): dict containing word and their respective vectors\n",
    "        embedding_dim (int): dimention of word vector\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    nb_words = len(tokenizer.word_index) + 1\n",
    "    word_index = tokenizer.word_index\n",
    "    embedding_matrix = np.zeros((nb_words, embedding_dim))\n",
    "    print(\"Embedding matrix shape: %s\" % str(embedding_matrix.shape))\n",
    "    for word, i in word_index.items():\n",
    "        try:\n",
    "            embedding_vector = word_vectors[word]\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "        except KeyError:\n",
    "            print(\"vector not found for word - %s\" % word)\n",
    "    print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))\n",
    "    return embedding_matrix\n",
    "\n",
    "\n",
    "def word_embed_meta_data(documents, embedding_dim):\n",
    "    \"\"\"\n",
    "    Load tokenizer object for given vocabs list\n",
    "    Args:\n",
    "        documents (list): list of document\n",
    "        embedding_dim (int): embedding dimension\n",
    "    Returns:\n",
    "        tokenizer (keras.preprocessing.text.Tokenizer): keras tokenizer object\n",
    "        embedding_matrix (dict): dict with word_index and vector mapping\n",
    "    \"\"\"\n",
    "    documents = [x.lower().split() for x in documents]\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(documents)\n",
    "    word_vector = train_word2vec(documents, embedding_dim)\n",
    "    embedding_matrix = create_embedding_matrix(tokenizer, word_vector, embedding_dim)\n",
    "    del word_vector\n",
    "    gc.collect()\n",
    "    return tokenizer, embedding_matrix\n",
    "\n",
    "\n",
    "def create_train_dev_set(tokenizer, sentences_pair, is_similar, max_sequence_length, validation_split_ratio):\n",
    "    \"\"\"\n",
    "    Create training and validation dataset\n",
    "    Args:\n",
    "        tokenizer (keras.preprocessing.text.Tokenizer): keras tokenizer object\n",
    "        sentences_pair (list): list of tuple of sentences pairs\n",
    "        is_similar (list): list containing labels if respective sentences in sentence1 and sentence2\n",
    "                           are same or not (1 if same else 0)\n",
    "        max_sequence_length (int): max sequence length of sentences to apply padding\n",
    "        validation_split_ratio (float): contain ratio to split training data into validation data\n",
    "    Returns:\n",
    "        train_data_1 (list): list of input features for training set from sentences1\n",
    "        train_data_2 (list): list of input features for training set from sentences2\n",
    "        labels_train (np.array): array containing similarity score for training data\n",
    "        leaks_train(np.array): array of training leaks features\n",
    "        val_data_1 (list): list of input features for validation set from sentences1\n",
    "        val_data_2 (list): list of input features for validation set from sentences1\n",
    "        labels_val (np.array): array containing similarity score for validation data\n",
    "        leaks_val (np.array): array of validation leaks features\n",
    "    \"\"\"\n",
    "    sentences1 = [x[0].lower() for x in sentences_pair]\n",
    "    sentences2 = [x[1].lower() for x in sentences_pair]\n",
    "    train_sequences_1 = tokenizer.texts_to_sequences(sentences1)\n",
    "    train_sequences_2 = tokenizer.texts_to_sequences(sentences2)\n",
    "    leaks = [[len(set(x1)), len(set(x2)), len(set(x1).intersection(x2))]\n",
    "             for x1, x2 in zip(train_sequences_1, train_sequences_2)]\n",
    "\n",
    "    train_padded_data_1 = pad_sequences(train_sequences_1, maxlen=max_sequence_length)\n",
    "    train_padded_data_2 = pad_sequences(train_sequences_2, maxlen=max_sequence_length)\n",
    "    train_labels = np.array(is_similar)\n",
    "    leaks = np.array(leaks)\n",
    "\n",
    "    shuffle_indices = np.random.permutation(np.arange(len(train_labels)))\n",
    "    train_data_1_shuffled = train_padded_data_1[shuffle_indices]\n",
    "    train_data_2_shuffled = train_padded_data_2[shuffle_indices]\n",
    "    train_labels_shuffled = train_labels[shuffle_indices]\n",
    "    leaks_shuffled = leaks[shuffle_indices]\n",
    "\n",
    "    dev_idx = max(1, int(len(train_labels_shuffled) * validation_split_ratio))\n",
    "\n",
    "    del train_padded_data_1\n",
    "    del train_padded_data_2\n",
    "    gc.collect()\n",
    "\n",
    "    train_data_1, val_data_1 = train_data_1_shuffled[:-dev_idx], train_data_1_shuffled[-dev_idx:]\n",
    "    train_data_2, val_data_2 = train_data_2_shuffled[:-dev_idx], train_data_2_shuffled[-dev_idx:]\n",
    "    labels_train, labels_val = train_labels_shuffled[:-dev_idx], train_labels_shuffled[-dev_idx:]\n",
    "    leaks_train, leaks_val = leaks_shuffled[:-dev_idx], leaks_shuffled[-dev_idx:]\n",
    "\n",
    "    return train_data_1, train_data_2, labels_train, leaks_train, val_data_1, val_data_2, labels_val, leaks_val\n",
    "\n",
    "\n",
    "def create_test_data(tokenizer, test_sentences_pair, max_sequence_length):\n",
    "    \"\"\"\n",
    "    Create training and validation dataset\n",
    "    Args:\n",
    "        tokenizer (keras.preprocessing.text.Tokenizer): keras tokenizer object\n",
    "        test_sentences_pair (list): list of tuple of sentences pairs\n",
    "        max_sequence_length (int): max sequence length of sentences to apply padding\n",
    "    Returns:\n",
    "        test_data_1 (list): list of input features for training set from sentences1\n",
    "        test_data_2 (list): list of input features for training set from sentences2\n",
    "    \"\"\"\n",
    "    test_sentences1 = [x[0].lower() for x in test_sentences_pair]\n",
    "    test_sentences2 = [x[1].lower() for x in test_sentences_pair]\n",
    "\n",
    "    test_sequences_1 = tokenizer.texts_to_sequences(test_sentences1)\n",
    "    test_sequences_2 = tokenizer.texts_to_sequences(test_sentences2)\n",
    "    leaks_test = [[len(set(x1)), len(set(x2)), len(set(x1).intersection(x2))]\n",
    "                  for x1, x2 in zip(test_sequences_1, test_sequences_2)]\n",
    "\n",
    "    leaks_test = np.array(leaks_test)\n",
    "    test_data_1 = pad_sequences(test_sequences_1, maxlen=max_sequence_length)\n",
    "    test_data_2 = pad_sequences(test_sequences_2, maxlen=max_sequence_length)\n",
    "\n",
    "    return test_data_1, test_data_2, leaks_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "6FA038D6A7FD4F39817A4225E17CC4DB",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding matrix shape: (529862, 200)\n",
      "Null word embeddings: 1\n",
      "529862\n",
      "time: 7min 18s\n"
     ]
    }
   ],
   "source": [
    "sentences1 = list(df_file['query'])\n",
    "sentences2 = list(df_file['title'])\n",
    "is_similar = list(df_file['label'])\n",
    "\n",
    "\n",
    "####################################\n",
    "######## Word Embedding ############\n",
    "####################################\n",
    "\n",
    "words_1 = list(set(list(df_file['query'])))\n",
    "#words_1 = list(set(list(df.concat([df_file['query'],test_file['query']],axis=0))))\n",
    "#words_2  = list(df.concat([df_file['title'],test_file['title']],axis=0))\n",
    "# creating word embedding meta data for word embedding \n",
    "tokenizer, embedding_matrix = word_embed_meta_data(words_1,200) #+ words_2,  100)\n",
    "print(len(embedding_matrix))\n",
    "embedding_meta_data = {\n",
    "\t'tokenizer': tokenizer,\n",
    "\t'embedding_matrix': embedding_matrix\n",
    "}\n",
    "\n",
    "## creating sentence pairs\n",
    "sentences_pair = [(x1, x2) for x1, x2 in zip(sentences1, sentences2)]\n",
    "del sentences1\n",
    "del sentences2\n",
    "del words_1\n",
    "#del words_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "08B32A504FF44F2D932F95B3387DA5E4",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 741 µs\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIM = 200\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 20\n",
    "VALIDATION_SPLIT = 0.1\n",
    "\n",
    "RATE_DROP_DENSE = 0.2\n",
    "NUMBER_LSTM = 6\n",
    "NUMBER_DENSE_UNITS = 6\n",
    "ACTIVATION_FUNCTION = 'relu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "17A111E3DB5F44348A3213F344C7C847",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1h 11min 23s\n"
     ]
    }
   ],
   "source": [
    "train_data_x1, train_data_x2, train_labels, leaks_train, \\\n",
    "        val_data_x1, val_data_x2, val_labels, leaks_val = create_train_dev_set(tokenizer, sentences_pair,\n",
    "                                                                               is_similar, MAX_SEQUENCE_LENGTH,\n",
    "                                                                               VALIDATION_SPLIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "1281488434174292A80601F2F2961203",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 632 µs\n"
     ]
    }
   ],
   "source": [
    "if train_data_x1 is None:\n",
    "    print(\"++++ !! Failure: Unable to train model ++++\")\n",
    "nb_words = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "B8CB357ECB674DD4A25B7C7CEAEEBCDF",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 881 µs\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import  GlobalMaxPooling1D, Concatenate\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.layers.normalization import BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "39624B5D24F547798803CBFBF46333CA",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 20)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 20, 200)      105972400   input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 18, 64)       38464       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 17, 64)       51264       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 16, 64)       64064       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 18, 32)       19232       embedding_1[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 17, 32)       25632       embedding_1[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 16, 32)       32032       embedding_1[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, 3)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 64)           0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_3 (GlobalM (None, 64)           0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_5 (GlobalM (None, 64)           0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 32)           0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_4 (GlobalM (None, 32)           0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_6 (GlobalM (None, 32)           0           conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 3)            12          input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 291)          0           global_max_pooling1d_1[0][0]     \n",
      "                                                                 global_max_pooling1d_3[0][0]     \n",
      "                                                                 global_max_pooling1d_5[0][0]     \n",
      "                                                                 global_max_pooling1d_2[0][0]     \n",
      "                                                                 global_max_pooling1d_4[0][0]     \n",
      "                                                                 global_max_pooling1d_6[0][0]     \n",
      "                                                                 dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 291)          1164        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 6)            1752        batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 6)            24          dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 6)            0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1)            7           dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 106,206,047\n",
      "Trainable params: 106,205,453\n",
      "Non-trainable params: 594\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "time: 4.03 s\n"
     ]
    }
   ],
   "source": [
    "embedding_layer = Embedding(nb_words, EMBEDDING_DIM, weights=[embedding_matrix],\n",
    "                                    input_length=MAX_SEQUENCE_LENGTH, trainable=True)\n",
    "input_query = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "input_title = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_query = embedding_layer(input_query)\n",
    "embedded_title = embedding_layer(input_title)\n",
    "convs_query = []\n",
    "convs_title = []\n",
    "for kernel_size in[3,4,5]:\n",
    "    c_query = Conv1D(64, kernel_size, activation='relu')(embedded_query)\n",
    "    c_query = GlobalMaxPooling1D()(c_query)\n",
    "    c_title = Conv1D(32, kernel_size, activation='relu')(embedded_title)\n",
    "    c_title = GlobalMaxPooling1D()(c_title)\n",
    "    convs_query.append(c_query)\n",
    "    convs_title.append(c_title)\n",
    "\n",
    "convs_query.extend(convs_title)\n",
    "leaks_input = Input(shape=(leaks_train.shape[1],))\n",
    "leaks_dense = Dense(int(NUMBER_DENSE_UNITS/2), activation=ACTIVATION_FUNCTION)(leaks_input)\n",
    "convs_query.append(leaks_dense)\n",
    "merged = concatenate(convs_query)\n",
    "merged = BatchNormalization()(merged)\n",
    "dropout_x = Dropout(RATE_DROP_DENSE)(merged)\n",
    "merged = Dense(NUMBER_DENSE_UNITS, activation=ACTIVATION_FUNCTION)(merged)\n",
    "merged = BatchNormalization()(merged)\n",
    "merged = Dropout(RATE_DROP_DENSE)(merged)\n",
    "preds = Dense(1, activation='sigmoid')(merged)\n",
    "model = Model(inputs=[input_query, input_title, leaks_input], outputs=preds)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "0BD84110964E4AAFBF70AFA4E662CE61",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 44.9 ms\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import Nadam \n",
    "model.compile(loss='binary_crossentropy', optimizer='nadam', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "E11081A898E64716AACE554DE18A755A",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 5.66 s\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import TensorBoard\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import time\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=1)\n",
    "checkpoint_dir = './checkpoints/' + str(int(time.time())) + '/'\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n",
    "bst_model_path = checkpoint_dir + '.h5'\n",
    "model_checkpoint = ModelCheckpoint(bst_model_path, save_best_only=True, save_weights_only=False)\n",
    "tensorboard = TensorBoard(log_dir=checkpoint_dir + \"logs/{}\".format(time.time()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "C9C1506C5BEE4BBD8CC3A2040CA16AD9",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 90000000 samples, validate on 10000000 samples\n",
      "Epoch 1/10\n",
      "90000000/90000000 [==============================] - 3575s 40us/step - loss: 0.4631 - acc: 0.8178 - val_loss: 0.4557 - val_acc: 0.8181\n",
      "Epoch 2/10\n",
      "90000000/90000000 [==============================] - 3574s 40us/step - loss: 0.4537 - acc: 0.8181 - val_loss: 0.4543 - val_acc: 0.8182\n",
      "Epoch 3/10\n",
      "90000000/90000000 [==============================] - 3576s 40us/step - loss: 0.4487 - acc: 0.8183 - val_loss: 0.4522 - val_acc: 0.8183\n",
      "Epoch 4/10\n",
      "90000000/90000000 [==============================] - 3574s 40us/step - loss: 0.4435 - acc: 0.8189 - val_loss: 0.4516 - val_acc: 0.8184\n",
      "Epoch 5/10\n",
      "90000000/90000000 [==============================] - 3575s 40us/step - loss: 0.4377 - acc: 0.8197 - val_loss: 0.4520 - val_acc: 0.8183\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa5742737f0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 4h 58min 24s\n"
     ]
    }
   ],
   "source": [
    "model.fit([train_data_x1, train_data_x2, leaks_train], train_labels,\n",
    "                  validation_data=([val_data_x1, val_data_x2, leaks_val], val_labels),\n",
    "                  epochs=10, batch_size=25000, shuffle=True,verbose=1,callbacks=[early_stopping, model_checkpoint, tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "B1D3C192A9C5492C860706C5D911D46F",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 590 µs\n"
     ]
    }
   ],
   "source": [
    "del train_data_x1\n",
    "del train_data_x2\n",
    "del leaks_train\n",
    "del val_data_x1\n",
    "del val_data_x2\n",
    "del leaks_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "A4F96149FF7D4E14AA7D7D251027D7F6",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./checkpoints/1565537041/\n",
      "time: 715 µs\n"
     ]
    }
   ],
   "source": [
    "print(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "3D1A0487915549A68327F5043904DBE3",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 9.39 s\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "Data_path='/home/kesci/input/bytedance/'\n",
    "test_path=os.path.join(Data_path,\"bytedance_contest.final_2.csv\")\n",
    "from keras.models import load_model\n",
    "model = load_model(bst_model_path)\n",
    "#test_path=os.path.join(Data_path,\"test_final_part1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "80B165683BC04A83AA9D28C72F76EA0A",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 1001.36 Mb (34.4% reduction)\n",
      "time: 1min 53s\n"
     ]
    }
   ],
   "source": [
    "test_file=df.read_csv(test_path,header=None,nrows=50000000)\n",
    "test_file = reduce_mem_usage(test_file)\n",
    "test_file.columns=[\"query_id\",\"query\",\"query_title_id\",\"title\"]\n",
    "test_sentences_pair = [(x1, x2) for x1, x2 in zip(test_file['query'], test_file['title'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "0FA7B8A8A1A1450EB3AC5B2C320FA5B9",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 34min 39s\n"
     ]
    }
   ],
   "source": [
    "test_data_x1, test_data_x2, leaks_test = create_test_data(tokenizer,test_sentences_pair,MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "4F13B8A4F48440038C02C8681DBE2C97",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000000/50000000 [==============================] - 2914s 58us/step\n",
      "time: 48min 36s\n"
     ]
    }
   ],
   "source": [
    "preds = list(model.predict([test_data_x1, test_data_x2, leaks_test], verbose=1).ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "1D16E47A1BFB4B1BA25E0EE7D3825602",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 9.49 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "sub=test_file[['query_id','query_title_id']]\n",
    "sub['prediction'] = preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "F84AAE5FFBA045CE84B126E8E8C8F958",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3min 53s\n"
     ]
    }
   ],
   "source": [
    "sub.to_csv('textcnn_final_part1.csv',index=False,header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "6D3A0B14CC9C4675A9F5D6034C3A0B0D",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 1001.36 Mb (34.4% reduction)\n",
      "time: 2min 15s\n"
     ]
    }
   ],
   "source": [
    "test_file=df.read_csv(test_path,header=None,nrows=50000000,skiprows=50000000)\n",
    "test_file = reduce_mem_usage(test_file)\n",
    "test_file.columns=[\"query_id\",\"query\",\"query_title_id\",\"title\"]\n",
    "test_sentences_pair = [(x1, x2) for x1, x2 in zip(test_file['query'], test_file['title'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "EEFE7FB2D7574E1E882D04CD79A1715C",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 36min 33s\n"
     ]
    }
   ],
   "source": [
    "test_data_x1, test_data_x2, leaks_test = create_test_data(tokenizer,test_sentences_pair,MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "3FDE257462464AF996177533C0971C87",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000000/50000000 [==============================] - 2913s 58us/step\n",
      "time: 48min 37s\n"
     ]
    }
   ],
   "source": [
    "preds = list(model.predict([test_data_x1, test_data_x2, leaks_test], verbose=1).ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "9E1CB03B45D6469286E891DBC69033A9",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 9.6 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "sub=test_file[['query_id','query_title_id']]\n",
    "sub['prediction'] = preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "CD9CADA12560430D90BA5CEF37FEB529",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 3min 49s\n"
     ]
    }
   ],
   "source": [
    "sub.to_csv('textcnn_final_part2.csv',index=False,header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "hide_input": false,
    "id": "34421D521D104C2592FA4C08750EDDEA",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 309 µs\n"
     ]
    }
   ],
   "source": [
    "#!https_proxy=\"http://klab-external-proxy\" ./kesci_submit -file textcnn2.csv -token df20f020d2454514"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9BF23008B64542C1845EB63C1951BD05",
    "scrolled": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
