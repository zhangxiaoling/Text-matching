{"cells":[{"outputs":[],"execution_count":1,"source":"# 显示cell运行时长\r\n%load_ext klab-autotime","cell_type":"code","metadata":{"trusted":true,"collapsed":false,"id":"FA9DBE722C554A2889FAD0D9B31CE788","scrolled":false}},{"outputs":[{"output_type":"stream","text":"time: 417 ms\n","name":"stdout"}],"execution_count":2,"source":"import os\r\nimport scipy.stats as stats\r\nimport pandas as df\r\nimport numpy as np","cell_type":"code","metadata":{"trusted":true,"collapsed":false,"id":"7A3B56A4A4BF42B79855296E6749DEC6","scrolled":false}},{"metadata":{"id":"E7E64D6A2205435A9DEA7886D941D646","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"Using TensorFlow backend.\n","name":"stderr"},{"output_type":"stream","text":"time: 1.44 s\n","name":"stdout"}],"source":"from sklearn.model_selection import train_test_split\r\nfrom keras.utils import to_categorical\r\nfrom keras.preprocessing import sequence\r\nfrom keras.layers import Embedding\r\nfrom keras.layers import Input, Dense, Multiply,Flatten,RepeatVector,Permute,Dropout\r\nfrom keras.layers import Conv1D,Bidirectional,LSTM\r\nfrom keras.models import Model","execution_count":3},{"outputs":[{"output_type":"stream","text":"time: 3.69 ms\n","name":"stdout"}],"execution_count":4,"source":"def reduce_mem_usage(df, verbose=True):\r\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\r\n    start_mem = df.memory_usage().sum() / 1024**2    \r\n    for col in df.columns:\r\n        col_type = df[col].dtypes\r\n        if col_type in numerics:\r\n            c_min = df[col].min()\r\n            c_max = df[col].max()\r\n            if str(col_type)[:3] == 'int':\r\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\r\n                    df[col] = df[col].astype(np.int8)\r\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\r\n                    df[col] = df[col].astype(np.int16)\r\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\r\n                    df[col] = df[col].astype(np.int32)\r\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\r\n                    df[col] = df[col].astype(np.int64)  \r\n            else:\r\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\r\n                    df[col] = df[col].astype(np.float16)\r\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\r\n                    df[col] = df[col].astype(np.float32)\r\n                else:\r\n                    df[col] = df[col].astype(np.float64)    \r\n    end_mem = df.memory_usage().sum() / 1024**2\r\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\r\n    return df","cell_type":"code","metadata":{"trusted":true,"collapsed":false,"id":"545F4D4BB7184F228EFD42E180288DE0","scrolled":false}},{"metadata":{"id":"5E4056A65DAA47E5881797F2A78BE8C7","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"time: 116 ms\n","name":"stdout"}],"source":"import os\r\nimport scipy.stats as stats\r\nimport pandas as pd\r\nimport gc\r\nimport numpy as np\r\nfrom gensim.models.word2vec import Word2Vec\r\nfrom gensim.models.word2vec import LineSentence\r\nfrom sklearn.externals import joblib\r\nfrom gensim.models import Doc2Vec\r\nimport pickle","execution_count":5},{"metadata":{"id":"71534B96E37446FFB54502D8A0271CB9","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"Mem. usage decreased to 1888.28 Mb (45.0% reduction)\ntime: 15min 9s\n","name":"stdout"}],"source":"Data_path='/home/kesci/input/bytedance/'\r\ntrain_path=os.path.join(Data_path,\"train_final.csv\")\r\ndf_file=df.read_csv(train_path,header=None,skiprows=910000000,nrows =90000000)\r\ndf_file = reduce_mem_usage(df_file)","execution_count":6},{"metadata":{"id":"CD41B3CC9DB1427597F243727032C05A","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"time: 971 µs\n","name":"stdout"}],"source":"df_file.columns=[\"query_id\",\"query\",\"query_title_id\",\"title\",\"label\"]","execution_count":7},{"metadata":{"id":"F1E6EBF9B51E42018A7910435334840C","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"time: 8.43 ms\n","name":"stdout"}],"source":"from keras.preprocessing.sequence import pad_sequences\r\nfrom keras.preprocessing.text import Tokenizer\r\nfrom gensim.models import Word2Vec\r\nimport numpy as np\r\nimport gc\r\n\r\n\r\ndef train_word2vec(documents, embedding_dim):\r\n    \"\"\"\r\n    train word2vector over traning documents\r\n    Args:\r\n        documents (list): list of document\r\n        embedding_dim (int): outpu wordvector size\r\n    Returns:\r\n        word_vectors(dict): dict containing words and their respective vectors\r\n    \"\"\"\r\n    model = Word2Vec(documents, workers=16,min_count=1, size=embedding_dim)\r\n    word_vectors = model.wv\r\n    del model\r\n    return word_vectors\r\n\r\n\r\ndef create_embedding_matrix(tokenizer, word_vectors, embedding_dim):\r\n    \"\"\"\r\n    Create embedding matrix containing word indexes and respective vectors from word vectors\r\n    Args:\r\n        tokenizer (keras.preprocessing.text.Tokenizer): keras tokenizer object containing word indexes\r\n        word_vectors (dict): dict containing word and their respective vectors\r\n        embedding_dim (int): dimention of word vector\r\n    Returns:\r\n    \"\"\"\r\n    nb_words = len(tokenizer.word_index) + 1\r\n    word_index = tokenizer.word_index\r\n    embedding_matrix = np.zeros((nb_words, embedding_dim))\r\n    print(\"Embedding matrix shape: %s\" % str(embedding_matrix.shape))\r\n    for word, i in word_index.items():\r\n        try:\r\n            embedding_vector = word_vectors[word]\r\n            if embedding_vector is not None:\r\n                embedding_matrix[i] = embedding_vector\r\n        except KeyError:\r\n            print(\"vector not found for word - %s\" % word)\r\n    print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))\r\n    return embedding_matrix\r\n\r\n\r\ndef word_embed_meta_data(documents, embedding_dim):\r\n    \"\"\"\r\n    Load tokenizer object for given vocabs list\r\n    Args:\r\n        documents (list): list of document\r\n        embedding_dim (int): embedding dimension\r\n    Returns:\r\n        tokenizer (keras.preprocessing.text.Tokenizer): keras tokenizer object\r\n        embedding_matrix (dict): dict with word_index and vector mapping\r\n    \"\"\"\r\n    documents = [x.lower().split() for x in documents]\r\n    tokenizer = Tokenizer()\r\n    tokenizer.fit_on_texts(documents)\r\n    word_vector = train_word2vec(documents, embedding_dim)\r\n    embedding_matrix = create_embedding_matrix(tokenizer, word_vector, embedding_dim)\r\n    del word_vector\r\n    gc.collect()\r\n    return tokenizer, embedding_matrix\r\n\r\n\r\ndef create_train_dev_set(tokenizer, sentences_pair, is_similar, max_sequence_length, validation_split_ratio):\r\n    \"\"\"\r\n    Create training and validation dataset\r\n    Args:\r\n        tokenizer (keras.preprocessing.text.Tokenizer): keras tokenizer object\r\n        sentences_pair (list): list of tuple of sentences pairs\r\n        is_similar (list): list containing labels if respective sentences in sentence1 and sentence2\r\n                           are same or not (1 if same else 0)\r\n        max_sequence_length (int): max sequence length of sentences to apply padding\r\n        validation_split_ratio (float): contain ratio to split training data into validation data\r\n    Returns:\r\n        train_data_1 (list): list of input features for training set from sentences1\r\n        train_data_2 (list): list of input features for training set from sentences2\r\n        labels_train (np.array): array containing similarity score for training data\r\n        leaks_train(np.array): array of training leaks features\r\n        val_data_1 (list): list of input features for validation set from sentences1\r\n        val_data_2 (list): list of input features for validation set from sentences1\r\n        labels_val (np.array): array containing similarity score for validation data\r\n        leaks_val (np.array): array of validation leaks features\r\n    \"\"\"\r\n    sentences1 = [x[0].lower() for x in sentences_pair]\r\n    sentences2 = [x[1].lower() for x in sentences_pair]\r\n    train_sequences_1 = tokenizer.texts_to_sequences(sentences1)\r\n    train_sequences_2 = tokenizer.texts_to_sequences(sentences2)\r\n    leaks = [[len(set(x1)), len(set(x2)), len(set(x1).intersection(x2))]\r\n             for x1, x2 in zip(train_sequences_1, train_sequences_2)]\r\n\r\n    train_padded_data_1 = pad_sequences(train_sequences_1, maxlen=max_sequence_length)\r\n    train_padded_data_2 = pad_sequences(train_sequences_2, maxlen=max_sequence_length)\r\n    train_labels = np.array(is_similar)\r\n    leaks = np.array(leaks)\r\n    \r\n    \r\n    train1= open('train_feature_first.pkl', 'rb')\r\n    train_feature = np.array(pickle.load(train1))\r\n    train2= open('train_feature.pkl', 'rb')\r\n    train_feature2 = np.array(pickle.load(train2))\r\n    train_feature=np.concatenate((train_feature[10000000:],train_feature2), axis=0)\r\n    del train_feature2\r\n    leaks=np.concatenate((leaks,train_feature), axis=1)\r\n    del train_feature\r\n    gc.collect()\r\n\r\n    shuffle_indices = np.random.permutation(np.arange(len(train_labels)))\r\n    train_data_1_shuffled = train_padded_data_1[shuffle_indices]\r\n    train_data_2_shuffled = train_padded_data_2[shuffle_indices]\r\n    train_labels_shuffled = train_labels[shuffle_indices]\r\n    leaks_shuffled = leaks[shuffle_indices]\r\n\r\n    dev_idx = max(1, int(len(train_labels_shuffled) * validation_split_ratio))\r\n\r\n    del train_padded_data_1\r\n    del train_padded_data_2\r\n    gc.collect()\r\n\r\n    train_data_1, val_data_1 = train_data_1_shuffled[:-dev_idx], train_data_1_shuffled[-dev_idx:]\r\n    train_data_2, val_data_2 = train_data_2_shuffled[:-dev_idx], train_data_2_shuffled[-dev_idx:]\r\n    labels_train, labels_val = train_labels_shuffled[:-dev_idx], train_labels_shuffled[-dev_idx:]\r\n    leaks_train, leaks_val = leaks_shuffled[:-dev_idx], leaks_shuffled[-dev_idx:]\r\n\r\n    return train_data_1, train_data_2, labels_train, leaks_train, val_data_1, val_data_2, labels_val, leaks_val\r\n\r\n\r\ndef create_test_data(tokenizer, test_sentences_pair, max_sequence_length):\r\n    \"\"\"\r\n    Create training and validation dataset\r\n    Args:\r\n        tokenizer (keras.preprocessing.text.Tokenizer): keras tokenizer object\r\n        test_sentences_pair (list): list of tuple of sentences pairs\r\n        max_sequence_length (int): max sequence length of sentences to apply padding\r\n    Returns:\r\n        test_data_1 (list): list of input features for training set from sentences1\r\n        test_data_2 (list): list of input features for training set from sentences2\r\n    \"\"\"\r\n    test_sentences1 = [x[0].lower() for x in test_sentences_pair]\r\n    test_sentences2 = [x[1].lower() for x in test_sentences_pair]\r\n\r\n    test_sequences_1 = tokenizer.texts_to_sequences(test_sentences1)\r\n    test_sequences_2 = tokenizer.texts_to_sequences(test_sentences2)\r\n    leaks_test = [[len(set(x1)), len(set(x2)), len(set(x1).intersection(x2))]\r\n                  for x1, x2 in zip(test_sequences_1, test_sequences_2)]\r\n\r\n    leaks_test = np.array(leaks_test)\r\n    test_data_1 = pad_sequences(test_sequences_1, maxlen=max_sequence_length)\r\n    test_data_2 = pad_sequences(test_sequences_2, maxlen=max_sequence_length)\r\n\r\n    return test_data_1, test_data_2, leaks_test","execution_count":8},{"metadata":{"id":"D0EB84638DA542FFB41D405CE20C56C5","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"Embedding matrix shape: (511560, 200)\nNull word embeddings: 1\n511560\ntime: 6min 29s\n","name":"stdout"}],"source":"sentences1 = list(df_file['query'])\r\nsentences2 = list(df_file['title'])\r\nis_similar = list(df_file['label'])\r\n\r\n\r\n####################################\r\n######## Word Embedding ############\r\n####################################\r\n\r\nwords_1 = list(set(list(df_file['query'])))\r\n#words_1 = list(set(list(df.concat([df_file['query'],test_file['query']],axis=0))))\r\n#words_2  = list(df.concat([df_file['title'],test_file['title']],axis=0))\r\n# creating word embedding meta data for word embedding \r\ntokenizer, embedding_matrix = word_embed_meta_data(words_1,200) #+ words_2,  100)\r\nprint(len(embedding_matrix))\r\nembedding_meta_data = {\r\n\t'tokenizer': tokenizer,\r\n\t'embedding_matrix': embedding_matrix\r\n}\r\n\r\n## creating sentence pairs\r\nsentences_pair = [(x1, x2) for x1, x2 in zip(sentences1, sentences2)]\r\ndel sentences1\r\ndel sentences2\r\ndel words_1\r\n#del words_2","execution_count":9},{"metadata":{"id":"0F5107D029ED494A89C323963CD5A327","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"time: 1.34 ms\n","name":"stdout"}],"source":"EMBEDDING_DIM = 200\r\n\r\nMAX_SEQUENCE_LENGTH = 20\r\nVALIDATION_SPLIT = 0.1\r\n\r\nRATE_DROP_DENSE = 0.2\r\nNUMBER_LSTM = 6\r\nNUMBER_DENSE_UNITS = 6\r\nACTIVATION_FUNCTION = 'relu'","execution_count":10},{"metadata":{"id":"262E46F877B54EFC96AD79BC51E77197","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"time: 1h 4min 6s\n","name":"stdout"}],"source":"train_data_x1, train_data_x2, train_labels, leaks_train, \\\r\n        val_data_x1, val_data_x2, val_labels, leaks_val = create_train_dev_set(tokenizer, sentences_pair,\r\n                                                                               is_similar, MAX_SEQUENCE_LENGTH,\r\n                                                                               VALIDATION_SPLIT)","execution_count":11},{"metadata":{"id":"3E268640E0F9479C86A8D868E3FF314C","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"time: 620 µs\n","name":"stdout"}],"source":"if train_data_x1 is None:\r\n    print(\"++++ !! Failure: Unable to train model ++++\")\r\nnb_words = len(tokenizer.word_index) + 1","execution_count":12},{"metadata":{"id":"2E8B2D47CE2A46C48D026E274DDCFF28","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"time: 909 µs\n","name":"stdout"}],"source":"from keras.layers import  GlobalMaxPooling1D, Concatenate\r\nfrom keras.layers.merge import concatenate\r\nfrom keras.layers.normalization import BatchNormalization","execution_count":13},{"metadata":{"id":"CC9F6891082A428BAC0AB7AA559575BE","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_1 (InputLayer)            (None, 20)           0                                            \n__________________________________________________________________________________________________\ninput_2 (InputLayer)            (None, 20)           0                                            \n__________________________________________________________________________________________________\nembedding_1 (Embedding)         (None, 20, 200)      102312000   input_1[0][0]                    \n                                                                 input_2[0][0]                    \n__________________________________________________________________________________________________\nconv1d_1 (Conv1D)               (None, 18, 64)       38464       embedding_1[0][0]                \n__________________________________________________________________________________________________\nconv1d_3 (Conv1D)               (None, 17, 64)       51264       embedding_1[0][0]                \n__________________________________________________________________________________________________\nconv1d_5 (Conv1D)               (None, 16, 64)       64064       embedding_1[0][0]                \n__________________________________________________________________________________________________\nconv1d_2 (Conv1D)               (None, 18, 32)       19232       embedding_1[1][0]                \n__________________________________________________________________________________________________\nconv1d_4 (Conv1D)               (None, 17, 32)       25632       embedding_1[1][0]                \n__________________________________________________________________________________________________\nconv1d_6 (Conv1D)               (None, 16, 32)       32032       embedding_1[1][0]                \n__________________________________________________________________________________________________\ninput_3 (InputLayer)            (None, 14)           0                                            \n__________________________________________________________________________________________________\nglobal_max_pooling1d_1 (GlobalM (None, 64)           0           conv1d_1[0][0]                   \n__________________________________________________________________________________________________\nglobal_max_pooling1d_3 (GlobalM (None, 64)           0           conv1d_3[0][0]                   \n__________________________________________________________________________________________________\nglobal_max_pooling1d_5 (GlobalM (None, 64)           0           conv1d_5[0][0]                   \n__________________________________________________________________________________________________\nglobal_max_pooling1d_2 (GlobalM (None, 32)           0           conv1d_2[0][0]                   \n__________________________________________________________________________________________________\nglobal_max_pooling1d_4 (GlobalM (None, 32)           0           conv1d_4[0][0]                   \n__________________________________________________________________________________________________\nglobal_max_pooling1d_6 (GlobalM (None, 32)           0           conv1d_6[0][0]                   \n__________________________________________________________________________________________________\ndense_1 (Dense)                 (None, 3)            45          input_3[0][0]                    \n__________________________________________________________________________________________________\nconcatenate_1 (Concatenate)     (None, 291)          0           global_max_pooling1d_1[0][0]     \n                                                                 global_max_pooling1d_3[0][0]     \n                                                                 global_max_pooling1d_5[0][0]     \n                                                                 global_max_pooling1d_2[0][0]     \n                                                                 global_max_pooling1d_4[0][0]     \n                                                                 global_max_pooling1d_6[0][0]     \n                                                                 dense_1[0][0]                    \n__________________________________________________________________________________________________\nbatch_normalization_1 (BatchNor (None, 291)          1164        concatenate_1[0][0]              \n__________________________________________________________________________________________________\ndense_2 (Dense)                 (None, 6)            1752        batch_normalization_1[0][0]      \n__________________________________________________________________________________________________\nbatch_normalization_2 (BatchNor (None, 6)            24          dense_2[0][0]                    \n__________________________________________________________________________________________________\ndropout_2 (Dropout)             (None, 6)            0           batch_normalization_2[0][0]      \n__________________________________________________________________________________________________\ndense_3 (Dense)                 (None, 1)            7           dropout_2[0][0]                  \n==================================================================================================\nTotal params: 102,545,680\nTrainable params: 102,545,086\nNon-trainable params: 594\n__________________________________________________________________________________________________\nNone\ntime: 4.21 s\n","name":"stdout"}],"source":"embedding_layer = Embedding(nb_words, EMBEDDING_DIM, weights=[embedding_matrix],\r\n                                    input_length=MAX_SEQUENCE_LENGTH, trainable=True)\r\ninput_query = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\r\ninput_title = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\r\nembedded_query = embedding_layer(input_query)\r\nembedded_title = embedding_layer(input_title)\r\nconvs_query = []\r\nconvs_title = []\r\nfor kernel_size in[3,4,5]:\r\n    c_query = Conv1D(64, kernel_size, activation='relu')(embedded_query)\r\n    c_query = GlobalMaxPooling1D()(c_query)\r\n    c_title = Conv1D(32, kernel_size, activation='relu')(embedded_title)\r\n    c_title = GlobalMaxPooling1D()(c_title)\r\n    convs_query.append(c_query)\r\n    convs_title.append(c_title)\r\n\r\nconvs_query.extend(convs_title)\r\nleaks_input = Input(shape=(leaks_train.shape[1],))\r\nleaks_dense = Dense(int(NUMBER_DENSE_UNITS/2), activation=ACTIVATION_FUNCTION)(leaks_input)\r\nconvs_query.append(leaks_dense)\r\nmerged = concatenate(convs_query)\r\nmerged = BatchNormalization()(merged)\r\ndropout_x = Dropout(RATE_DROP_DENSE)(merged)\r\nmerged = Dense(NUMBER_DENSE_UNITS, activation=ACTIVATION_FUNCTION)(merged)\r\nmerged = BatchNormalization()(merged)\r\nmerged = Dropout(RATE_DROP_DENSE)(merged)\r\npreds = Dense(1, activation='sigmoid')(merged)\r\nmodel = Model(inputs=[input_query, input_title, leaks_input], outputs=preds)\r\nprint(model.summary())","execution_count":14},{"metadata":{"id":"B824154CEEC74440947C818A8441BDA8","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"time: 46.5 ms\n","name":"stdout"}],"source":"from keras.optimizers import Nadam \r\nmodel.compile(loss='binary_crossentropy', optimizer='nadam', metrics=['acc'])","execution_count":15},{"metadata":{"id":"F0B59731395E43DD8B055A86FF454417","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"time: 5.65 s\n","name":"stdout"}],"source":"from keras.callbacks import TensorBoard\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nimport time\nearly_stopping = EarlyStopping(monitor='val_loss', patience=1)\ncheckpoint_dir = './checkpoints/' + str(int(time.time())) + '/'\nif not os.path.exists(checkpoint_dir):\n    os.makedirs(checkpoint_dir)\nbst_model_path = checkpoint_dir + '.h5'\nmodel_checkpoint = ModelCheckpoint(bst_model_path, save_best_only=True, save_weights_only=False)\ntensorboard = TensorBoard(log_dir=checkpoint_dir + \"logs/{}\".format(time.time()))","execution_count":16},{"metadata":{"id":"41A7A51B54B74A80B8872F5F7E5BE033","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:109: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 102312000 elements. This may consume a large amount of memory.\n  num_elements)\n","name":"stderr"},{"output_type":"stream","text":"Train on 81000000 samples, validate on 9000000 samples\nEpoch 1/10\n81000000/81000000 [==============================] - 3316s 41us/step - loss: 0.4667 - acc: 0.8138 - val_loss: 0.4543 - val_acc: 0.8179\nEpoch 2/10\n81000000/81000000 [==============================] - 3310s 41us/step - loss: 0.4514 - acc: 0.8180 - val_loss: 0.4514 - val_acc: 0.8181\nEpoch 3/10\n81000000/81000000 [==============================] - 3309s 41us/step - loss: 0.4460 - acc: 0.8183 - val_loss: 0.4508 - val_acc: 0.8182\nEpoch 4/10\n81000000/81000000 [==============================] - 3310s 41us/step - loss: 0.4401 - acc: 0.8190 - val_loss: 0.4506 - val_acc: 0.8182\nEpoch 5/10\n81000000/81000000 [==============================] - 3312s 41us/step - loss: 0.4334 - acc: 0.8202 - val_loss: 0.4514 - val_acc: 0.8175\n","name":"stdout"},{"output_type":"execute_result","metadata":{},"data":{"text/plain":"<keras.callbacks.History at 0x7f9ee2156c88>"},"execution_count":17},{"output_type":"stream","text":"time: 4h 36min 32s\n","name":"stdout"}],"source":"model.fit([train_data_x1, train_data_x2, leaks_train], train_labels,\r\n                  validation_data=([val_data_x1, val_data_x2, leaks_val], val_labels),\r\n                  epochs=10, batch_size=20480, shuffle=True,verbose=1,callbacks=[early_stopping, model_checkpoint, tensorboard])","execution_count":17},{"metadata":{"id":"2B764ED72DAB47F98DE926CE523ECA10","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"time: 606 µs\n","name":"stdout"}],"source":"del train_data_x1\ndel train_data_x2\ndel leaks_train\ndel val_data_x1\ndel val_data_x2\ndel leaks_val","execution_count":18},{"metadata":{"id":"EA643AFFE4CB4130964432F2EA1ACCF3","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"./checkpoints/1565246039/\ntime: 1.22 ms\n","name":"stdout"}],"source":"print(checkpoint_dir)","execution_count":19},{"metadata":{"id":"4E74FFDB01B742F381770A2582C3E4BE","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"Mem. usage decreased to 400.54 Mb (34.4% reduction)\ntime: 44.2 s\n","name":"stdout"}],"source":"test_path=os.path.join(Data_path,\"test_final_part1.csv\")\r\ntest_file=df.read_csv(test_path,header=None)\r\ntest_file = reduce_mem_usage(test_file)\r\ntest_file.columns=[\"query_id\",\"query\",\"query_title_id\",\"title\"]\r\ntest_sentences_pair = [(x1, x2) for x1, x2 in zip(test_file['query'], test_file['title'])]","execution_count":20},{"metadata":{"id":"3ED8BC357AB44DB992E75328E8B72579","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"time: 13min 19s\n","name":"stdout"}],"source":"test_data_x1, test_data_x2, leaks_test = create_test_data(tokenizer,test_sentences_pair,MAX_SEQUENCE_LENGTH)","execution_count":21},{"metadata":{"id":"058D7632E1BA406E8BBD897EE15327D9","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"time: 5.9 s\n","name":"stdout"}],"source":"test= open('test_feature.pkl', 'rb')\r\ntest_feature = np.array(pickle.load(test))\r\nleaks_test=np.concatenate((leaks_test,test_feature), axis=1)","execution_count":22},{"metadata":{"id":"1107AEF138154DF28017107F2EEBC9CB","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"20000000/20000000 [==============================] - 1168s 58us/step\ntime: 19min 38s\n","name":"stdout"}],"source":"from keras.models import load_model\nmodel = load_model(bst_model_path)\npreds = list(model.predict([test_data_x1, test_data_x2, leaks_test], verbose=1).ravel())","execution_count":23},{"metadata":{"id":"484AE493F9E745BC928EF1D17FF99A91","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"time: 4.64 s\n","name":"stdout"},{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n  \n","name":"stderr"}],"source":"sub=test_file[['query_id','query_title_id']]\r\nsub['prediction'] = preds","execution_count":24},{"metadata":{"id":"43EFA0E4AF004C88BFA1C9064A8C93AC","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"time: 1min 29s\n","name":"stdout"}],"source":"sub.to_csv('textcnn.csv',index=False,header=False)","execution_count":25},{"metadata":{"id":"ED152DE5C75C4EE889162099317B1ADC","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"Kesci Submit Tool 3.0\n\n> 已验证Token\n> 提交文件 textcnn.csv (575561.33 KiB)\n> 文件已上传        \n> 提交完成\ntime: 24.9 s\n","name":"stdout"}],"source":"!https_proxy=\"http://klab-external-proxy\" ./kesci_submit -file textcnn.csv -token 9dd65ea29b2ce893","execution_count":26},{"metadata":{"id":"2047F75ADA2F48F6BAEF640C5EBC0C01","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"","execution_count":null}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":0}