{"cells":[{"outputs":[],"execution_count":1,"source":"%load_ext klab-autotime","cell_type":"code","metadata":{"trusted":true,"collapsed":false,"id":"5378E2B284F34E9885FD42681651A515","scrolled":false}},{"outputs":[{"output_type":"stream","text":"time: 859 ms\n","name":"stdout"}],"execution_count":2,"source":"import argparse\r\nimport functools\r\nfrom collections import defaultdict\r\n\r\nimport numpy as np\r\nimport pandas as df\r\n\r\nfrom nltk.corpus import stopwords\r\nfrom collections import Counter\r\nfrom sklearn.metrics import log_loss\r\nfrom tqdm import trange\r\nfrom tqdm import tqdm, tqdm_notebook\r\ntqdm.pandas()\r\ndef word_match_share(row, stops=None):\r\n    q1words = {}\r\n    q2words = {}\r\n    for word in row['title']:\r\n        if word not in stops:\r\n            q2words[word] = 1\r\n    for word in row['qurey']:\r\n        if word not in stops:\r\n            q1words[word] = 1\r\n    if len(q1words) == 0 or len(q2words) == 0:\r\n        # The computer-generated chaff includes a few questions that are nothing but stopwords\r\n        return 0\r\n    shared_words_in_q1 = [w for w in q1words.keys() if w in q2words]\r\n    shared_words_in_q2 = [w for w in q2words.keys() if w in q1words]\r\n    R = (len(shared_words_in_q1) + len(shared_words_in_q2))/(len(q1words) + len(q2words))\r\n    return R\r\n\r\ndef jaccard(row):\r\n    wic = set(row['qurey']).intersection(set(row['title']))\r\n    uw = set(row['qurey']).union(row['title'])\r\n    if len(uw) == 0:\r\n        uw = [1]\r\n    return (len(wic) / len(uw))\r\n\r\ndef common_words(row):\r\n    return len(set(row['qurey']).intersection(set(row['title'])))\r\n    \r\ndef len_query(row):\r\n    return len(row['qurey'])\r\n\r\ndef len_title(row):\r\n    return len(row['title'])\r\n\r\ndef total_unique_words(row):\r\n    return len(set(row['qurey']).union(row['title']))\r\n\r\ndef total_unq_words_stop(row, stops):\r\n    return len([x for x in set(row['qurey']).union(row['title']) if x not in stops])\r\n\r\ndef wc_diff(row):\r\n    return abs(len(row['qurey']) - len(row['title']))\r\n\r\ndef wc_ratio(row):\r\n    l1 = len(row['qurey'])*1.0 \r\n    l2 = len(row['title'])\r\n    if l2 == 0:\r\n        return np.nan\r\n    if l1 / l2:\r\n        return l2 / l1\r\n    else:\r\n        return l1 / l2\r\n\r\ndef wc_diff_unique(row):\r\n    return abs(len(set(row['qurey'])) - len(set(row['title'])))\r\n\r\ndef wc_ratio_unique(row):\r\n    l1 = len(set(row['qurey'])) * 1.0\r\n    l2 = len(set(row['title']))\r\n    if l2 == 0:\r\n        return np.nan\r\n    if l1 / l2:\r\n        return l2 / l1\r\n    else:\r\n        return l1 / l2\r\n\r\ndef wc_diff_unique_stop(row, stops=None):\r\n    return abs(len([x for x in set(row['qurey']) if x not in stops]) - len([x for x in set(row['title']) if x not in stops]))\r\n\r\ndef wc_ratio_unique_stop(row, stops=None):\r\n    l1 = len([x for x in set(row['qurey']) if x not in stops])*1.0 \r\n    l2 = len([x for x in set(row['title']) if x not in stops])\r\n    if l2 == 0:\r\n        return np.nan\r\n    if l1 / l2:\r\n        return l2 / l1\r\n    else:\r\n        return l1 / l2\r\n\r\ndef same_start_word(row):\r\n    if not row['qurey'] or not row['title']:\r\n        return np.nan\r\n    return int(row['qurey'][0] == row['title'][0])\r\n\r\ndef char_diff(row):\r\n    return abs(len(''.join(row['qurey'])) - len(''.join(row['title'])))\r\n\r\ndef char_ratio(row):\r\n    l1 = len(''.join(row['qurey'])) \r\n    l2 = len(''.join(row['title']))\r\n    if l2 == 0:\r\n        return np.nan\r\n    if l1 / l2:\r\n        return l2 / l1\r\n    else:\r\n        return l1 / l2\r\n\r\ndef char_diff_unique_stop(row, stops=None):\r\n    return abs(len(''.join([x for x in set(row['qurey']) if x not in stops])) - len(''.join([x for x in set(row['title']) if x not in stops])))\r\n\r\n\r\ndef get_weight(count, eps=10000, min_count=2):\r\n    if count < min_count:\r\n        return 0\r\n    else:\r\n        return 1 / (count + eps)\r\n    \r\ndef tfidf_word_match_share_stops(row, stops=None, weights=None):\r\n    q1words = {}\r\n    q2words = {}\r\n    for word in row['title']:\r\n        if word not in stops:\r\n            q2words[word] = 1\r\n    for word in row['qurey']:\r\n        if word not in stops:\r\n            q1words[word] = 1\r\n    if len(q1words) == 0 or len(q2words) == 0:\r\n        # The computer-generated chaff includes a few questions that are nothing but stopwords\r\n        return 0\r\n    \r\n    shared_weights = [weights.get(w, 0) for w in q1words.keys() if w in q2words] + [weights.get(w, 0) for w in q2words.keys() if w in q1words]\r\n    total_weights = [weights.get(w, 0) for w in q1words] + [weights.get(w, 0) for w in q2words]\r\n    \r\n    R = np.sum(shared_weights) / np.sum(total_weights)\r\n    return R\r\n\r\ndef tfidf_word_match_share(row, weights=None):\r\n    q1words = {}\r\n    q2words = {}\r\n    for word in row['qurey']:\r\n        q1words[word] = 1\r\n    for word in row['title']:\r\n        q2words[word] = 1\r\n    if len(q1words) == 0 or len(q2words) == 0:\r\n        # The computer-generated chaff includes a few questions that are nothing but stopwords\r\n        return 0\r\n    \r\n    shared_weights = [weights.get(w, 0) for w in q1words.keys() if w in q2words] + [weights.get(w, 0) for w in q2words.keys() if w in q1words]\r\n    total_weights = [weights.get(w, 0) for w in q1words] + [weights.get(w, 0) for w in q2words]\r\n    \r\n    R = np.sum(shared_weights) / np.sum(total_weights)\r\n    return R\r\n\r\n\r\ndef build_features(data, stops, weights):\r\n    X = df.DataFrame()\r\n    f = functools.partial(word_match_share, stops=stops)\r\n    \r\n    \r\n    #X['len_query'] = data.apply(len_query, axis=1, raw=True)\r\n    #X['len_title'] = data.apply(len_title, axis=1, raw=True)\r\n    #X['common_words'] = data.apply(common_words, axis=1, raw=True)  #14\r\n    X['word_match'] = data.progress_apply(f, axis=1, raw=True) #1\r\n\r\n    f = functools.partial(tfidf_word_match_share, weights=weights)\r\n    X['tfidf_wm'] = data.progress_apply(f, axis=1, raw=True) #2\r\n\r\n    # f = functools.partial(tfidf_word_match_share_stops, stops=stops, weights=weights)\r\n    # X['tfidf_wm_stops'] = data.apply(f, axis=1, raw=True) #3\r\n\r\n    X['jaccard'] = data.progress_apply(jaccard, axis=1, raw=True) #4\r\n    X['wc_diff'] = data.progress_apply(wc_diff, axis=1, raw=True) #5\r\n    X['wc_ratio'] = data.progress_apply(wc_ratio, axis=1, raw=True) #6\r\n    X['wc_diff_unique'] = data.progress_apply(wc_diff_unique, axis=1, raw=True) #7\r\n    X['wc_ratio_unique'] = data.progress_apply(wc_ratio_unique, axis=1, raw=True) #8\r\n\r\n    # f = functools.partial(wc_diff_unique_stop, stops=stops)    \r\n    # X['wc_diff_unq_stop'] = data.apply(f, axis=1, raw=True) #9\r\n    # f = functools.partial(wc_ratio_unique_stop, stops=stops)    \r\n    # X['wc_ratio_unique_stop'] = data.apply(f, axis=1, raw=True) #10\r\n\r\n    X['same_start'] = data.progress_apply(same_start_word, axis=1, raw=True) #11\r\n    X['char_diff'] = data.progress_apply(char_diff, axis=1, raw=True) #12\r\n\r\n    # f = functools.partial(char_diff_unique_stop, stops=stops) \r\n    # X['char_diff_unq_stop'] = data.progress_apply(f, axis=1, raw=True) #13\r\n\r\n    X['total_unique_words'] = data.progress_apply(total_unique_words, axis=1, raw=True)  #15\r\n\r\n    # f = functools.partial(total_unq_words_stop, stops=stops)\r\n    # X['total_unq_words_stop'] = data.apply(f, axis=1, raw=True)  #16\r\n    \r\n    X['char_ratio'] = data.progress_apply(char_ratio, axis=1, raw=True) #17    \r\n\r\n    return X","cell_type":"code","metadata":{"trusted":true,"collapsed":false,"id":"4F27FAD8C0CE47B482FCD5C19F16B5D4","scrolled":false}},{"outputs":[{"output_type":"stream","text":"time: 3.83 ms\n","name":"stdout"}],"execution_count":3,"source":"def reduce_mem_usage(df, verbose=True):\r\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\r\n    start_mem = df.memory_usage().sum() / 1024**2    \r\n    for col in df.columns:\r\n        col_type = df[col].dtypes\r\n        if col_type in numerics:\r\n            c_min = df[col].min()\r\n            c_max = df[col].max()\r\n            if str(col_type)[:3] == 'int':\r\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\r\n                    df[col] = df[col].astype(np.int8)\r\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\r\n                    df[col] = df[col].astype(np.int16)\r\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\r\n                    df[col] = df[col].astype(np.int32)\r\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\r\n                    df[col] = df[col].astype(np.int64)  \r\n            else:\r\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\r\n                    df[col] = df[col].astype(np.float16)\r\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\r\n                    df[col] = df[col].astype(np.float32)\r\n                else:\r\n                    df[col] = df[col].astype(np.float64)    \r\n    end_mem = df.memory_usage().sum() / 1024**2\r\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\r\n    return df","cell_type":"code","metadata":{"trusted":true,"collapsed":false,"id":"A01E5F56AA1E44968BE1043983B7B333","scrolled":false}},{"outputs":[{"output_type":"stream","text":"Mem. usage decreased to 629.43 Mb (45.0% reduction)\n","name":"stdout"},{"output_type":"execute_result","metadata":{},"data":{"text/plain":"'\\ntest_path=os.path.join(Data_path,\"test_final_part1.csv\")\\ndf_test=df.read_csv(test_path,header=None)\\ndf_test = reduce_mem_usage(df_test)\\n'"},"execution_count":4},{"output_type":"stream","text":"time: 12min 46s\n","name":"stdout"}],"execution_count":4,"source":"import os\r\nData_path='/home/kesci/input/bytedance/'\r\ntrain_path=os.path.join(Data_path,\"train_final.csv\")\r\ndf_train=df.read_csv(train_path,header=None,skiprows=900000000,nrows =30000000)#,skiprows=980000000,nrows =20000000,skiprows=90000000\r\ndf_train = reduce_mem_usage(df_train)\r\n'''\r\ntest_path=os.path.join(Data_path,\"test_final_part1.csv\")\r\ndf_test=df.read_csv(test_path,header=None)\r\ndf_test = reduce_mem_usage(df_test)\r\n'''","cell_type":"code","metadata":{"trusted":true,"collapsed":false,"id":"3DE7F38E89A143D7838D39093A0C571A","scrolled":false}},{"metadata":{"id":"D5988D5F572F4A339B6A482E3404D784","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"time: 1 ms\n","name":"stdout"}],"source":"df_train.columns=[\"query_id\",\"qurey\",\"query_title_id\",\"title\",\"label\"]\r\n#df_test.columns=[\"query_id\",\"qurey\",\"query_title_id\",\"title\"]","execution_count":5},{"metadata":{"id":"545F3E7CD3C841848FE80EDAE6EBE644","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"execute_result","metadata":{},"data":{"text/plain":"\"\\ndf_test['qurey'] = df_test['qurey'].str.split()\\ndf_test['title'] = df_test['title'].str.split()\\ntest_qs = df.Series(df_test['qurey'].tolist() + df_test['title'].tolist())\\nwords = [x for y in test_qs for x in y]\\ncounts = Counter(words)\\nweights = {word: get_weight(count) for word, count in counts.items()}\\nprint('Building Features')\\nstops=[]\\nX_test = build_features(df_test, stops, weights)\\n\""},"execution_count":6},{"output_type":"stream","text":"time: 2.04 ms\n","name":"stdout"}],"source":"'''\r\ndf_test['qurey'] = df_test['qurey'].str.split()\r\ndf_test['title'] = df_test['title'].str.split()\r\ntest_qs = df.Series(df_test['qurey'].tolist() + df_test['title'].tolist())\r\nwords = [x for y in test_qs for x in y]\r\ncounts = Counter(words)\r\nweights = {word: get_weight(count) for word, count in counts.items()}\r\nprint('Building Features')\r\nstops=[]\r\nX_test = build_features(df_test, stops, weights)\r\n'''","execution_count":6},{"metadata":{"id":"A14818405E5E48D9BA1D0F50B833357E","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"\r  0%|          | 0/30000000 [00:00<?, ?it/s]","name":"stderr"},{"output_type":"stream","text":"Building Features\n","name":"stdout"},{"output_type":"stream","text":"100%|██████████| 30000000/30000000 [20:50<00:00, 23989.10it/s]\n100%|██████████| 30000000/30000000 [41:34<00:00, 12028.10it/s]\n100%|██████████| 30000000/30000000 [32:05<00:00, 15580.09it/s]\n100%|██████████| 30000000/30000000 [17:21<00:00, 28805.61it/s]\n100%|██████████| 30000000/30000000 [17:20<00:00, 28820.62it/s]\n100%|██████████| 30000000/30000000 [18:10<00:00, 27508.59it/s]\n100%|██████████| 30000000/30000000 [18:27<00:00, 27079.78it/s]\n100%|██████████| 30000000/30000000 [29:57<00:00, 16693.05it/s]\n100%|██████████| 30000000/30000000 [18:06<00:00, 27615.08it/s]\n100%|██████████| 30000000/30000000 [18:29<00:00, 27027.69it/s]\n100%|██████████| 30000000/30000000 [17:37<00:00, 28369.91it/s]","name":"stderr"},{"output_type":"stream","text":"time: 4h 13min 29s\n","name":"stdout"},{"output_type":"stream","text":"\n","name":"stderr"}],"source":"df_train['qurey'] = df_train['qurey'].str.split()\r\ndf_train['title'] = df_train['title'].str.split()\r\ntrain_qs = df.Series(df_train['qurey'].tolist() + df_train['title'].tolist())\r\nwords = [x for y in train_qs for x in y]\r\ncounts = Counter(words)\r\nweights = {word: get_weight(count) for word, count in counts.items()}\r\nprint('Building Features')\r\nstops=[]\r\nX_train = build_features(df_train, stops, weights)","execution_count":7},{"metadata":{"id":"C2F5767B991D45458B7601C7422B6794","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"Mem. usage decreased to 600.81 Mb (76.1% reduction)\ntime: 8.19 s\n","name":"stdout"}],"source":"X_train=reduce_mem_usage(X_train)\n#X_test=reduce_mem_usage(X_test)","execution_count":8},{"metadata":{"id":"31B71ED7BB13418B94B27CEBB2521BDA","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[{"output_type":"stream","text":"time: 1.29 s\n","name":"stdout"}],"source":"'''\r\ndata = open('test_feature.pkl', 'wb')\r\nimport pickle\r\npickle.dump(X_test, file=data)\r\n'''\r\ndata = open('train_feature_first.pkl', 'wb')\r\nimport pickle\r\npickle.dump(X_train, file=data)","execution_count":9},{"metadata":{"id":"773F2A6E3E3E4FB48E9615B231CE8CEB","collapsed":false,"scrolled":false},"cell_type":"code","outputs":[],"source":"","execution_count":null}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":0}